{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2025ddc2",
   "metadata": {},
   "source": [
    "# Hoja de trabajo 2\n",
    "\n",
    "Link del repositorio: https://github.com/faguilarleal/HDT2_deep  \n",
    "\n",
    "\n",
    "Integrantes:  \n",
    "- Franci Aguilar 22243\n",
    "- César López 22404\n",
    "\n",
    "\n",
    "## Ejercicio 1. Experimentación práctica\n",
    "#### Task 1 - Preparación del conjunto de datos\n",
    "Cargue el conjunto de datos de Iris utilizando bibliotecas como sklearn.datasets. Luego, divida el conjunto de datos en conjuntos de entrenamiento y validación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee791006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d745e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "Tamaños de los conjuntos:\n",
      "Entrenamiento: (120, 4) (120,)\n",
      "Validación: (30, 4) (30,)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el conjunto de datos Iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Convertir a DataFrame para verlo más ordenado\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "\n",
    "print(\"Primeras filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTamaños de los conjuntos:\")\n",
    "print(\"Entrenamiento:\", X_train.shape, y_train.shape)\n",
    "print(\"Validación:\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd4fff",
   "metadata": {},
   "source": [
    "#### Task 2 - Arquitectura modelo\n",
    "Cree una red neuronal feedforward simple utilizando nn.Module de PyTorch. Luego, defina capa de entrada, capas ocultas y capa de salida. Después, elija las funciones de activación y el número de neuronas por capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24de36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisNet(\n",
      "  (fc1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNet, self).__init__()\n",
    "        # Capa de entrada -> 4 neuronas \n",
    "        # Primera capa oculta -> 16 neuronas\n",
    "        # Segunda capa oculta -> 8 neuronas\n",
    "        # Capa de salida -> 3 neuronas (porque iris tiene 3 clases)\n",
    "        self.fc1 = nn.Linear(4, 16)   # entrada -> capa oculta 1\n",
    "        self.fc2 = nn.Linear(16, 8)   # capa oculta 1 -> capa oculta 2\n",
    "        self.fc3 = nn.Linear(8, 3)    # capa oculta 2 -> salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Funciones de activación\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = F.relu(self.fc2(x))  \n",
    "        x = self.fc3(x)         \n",
    "        return x\n",
    "\n",
    "# Instanciar el modelo\n",
    "model = IrisNet()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf12011",
   "metadata": {},
   "source": [
    "#### Task 3 - Funciones de Pérdida\n",
    "Utilice diferentes funciones de pérdida comunes como Cross-Entropy Loss y MSE para clasificación. Entrene el modelo con diferentes funciones de pérdida y registre las pérdidas de entrenamiento y test. Debe utilizar al menos 3 diferentes funciones. Es decir, procure que su código sea capaz de parametrizar el uso de diferentes funciones de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09cd6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CrossEntropy] Epoch 10/50 Train Loss: 0.9309 | Val Loss: 0.9104\n",
      "[CrossEntropy] Epoch 20/50 Train Loss: 0.6141 | Val Loss: 0.5871\n",
      "[CrossEntropy] Epoch 30/50 Train Loss: 0.3787 | Val Loss: 0.3689\n",
      "[CrossEntropy] Epoch 40/50 Train Loss: 0.2358 | Val Loss: 0.2347\n",
      "[CrossEntropy] Epoch 50/50 Train Loss: 0.1307 | Val Loss: 0.1371\n",
      "[MSE] Epoch 10/50 Train Loss: 0.3377 | Val Loss: 0.3335\n",
      "[MSE] Epoch 20/50 Train Loss: 0.2429 | Val Loss: 0.2261\n",
      "[MSE] Epoch 30/50 Train Loss: 0.1848 | Val Loss: 0.1767\n",
      "[MSE] Epoch 40/50 Train Loss: 0.1370 | Val Loss: 0.1334\n",
      "[MSE] Epoch 50/50 Train Loss: 0.1176 | Val Loss: 0.1178\n",
      "[NLLLoss] Epoch 10/50 Train Loss: 0.8780 | Val Loss: 0.8398\n",
      "[NLLLoss] Epoch 20/50 Train Loss: 0.5714 | Val Loss: 0.5461\n",
      "[NLLLoss] Epoch 30/50 Train Loss: 0.3606 | Val Loss: 0.3477\n",
      "[NLLLoss] Epoch 40/50 Train Loss: 0.2195 | Val Loss: 0.2180\n",
      "[NLLLoss] Epoch 50/50 Train Loss: 0.1300 | Val Loss: 0.1392\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "#  Funciones de pérdida disponibles\n",
    "loss_functions = {\n",
    "    \"CrossEntropy\": nn.CrossEntropyLoss(),\n",
    "    \"MSE\": nn.MSELoss(),  # MSE requiere one-hot en y\n",
    "    \"NLLLoss\": nn.NLLLoss()  # requiere log_softmax en salida\n",
    "}\n",
    "\n",
    "\n",
    "#  Entrenamiento parametrizable\n",
    "def train_model(loss_name, epochs=50, lr=0.01):\n",
    "    model = IrisNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = loss_functions[loss_name]\n",
    "\n",
    "    # Para NLL necesitamos modificar el forward (añadir log_softmax)\n",
    "    use_log_softmax = (loss_name == \"NLLLoss\")\n",
    "\n",
    "    # Para MSE necesitamos one-hot encoding de las etiquetas\n",
    "    if loss_name == \"MSE\":\n",
    "        y_train_oh = torch.nn.functional.one_hot(y_train, num_classes=3).float()\n",
    "        y_val_oh   = torch.nn.functional.one_hot(y_val, num_classes=3).float()\n",
    "    else:\n",
    "        y_train_oh, y_val_oh = y_train, y_val\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Forward ---\n",
    "        outputs = model(X_train)\n",
    "        if use_log_softmax:\n",
    "            outputs = torch.log_softmax(outputs, dim=1)\n",
    "\n",
    "        loss = criterion(outputs, y_train_oh)\n",
    "\n",
    "        # --- Backprop ---\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Evaluación en validación ---\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            if use_log_softmax:\n",
    "                val_outputs = torch.log_softmax(val_outputs, dim=1)\n",
    "            val_loss = criterion(val_outputs, y_val_oh)\n",
    "\n",
    "        history[\"train_loss\"].append(loss.item())\n",
    "        history[\"val_loss\"].append(val_loss.item())\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"[{loss_name}] Epoch {epoch+1}/{epochs} \"\n",
    "                  f\"Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Entrenar con las tres funciones\n",
    "# -------------------------------\n",
    "hist_ce  = train_model(\"CrossEntropy\", epochs=50)\n",
    "hist_mse = train_model(\"MSE\", epochs=50)\n",
    "hist_nll = train_model(\"NLLLoss\", epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b1d39",
   "metadata": {},
   "source": [
    "#### Task 4 - Técnicas de Regularización\n",
    "\n",
    "Utilice distintas técnicas de regularización como L1, L2 y dropout. Entrene el modelo con y sin técnicas de\n",
    "regularización y observe el impacto en el overfitting y la generalización. Debe utilizar al menos 3 diferentes técnicas.\n",
    "Es decir, procure que su código sea capaz de parametrizar el uso de diferentes técnicas de regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c54a36",
   "metadata": {},
   "source": [
    "#### Task 5 - Algoritmos de Optimización\n",
    "\n",
    "Utilice distintas técnicas de optimización como SGD, Batch GD, Mini-Batch GD. Entrene el modelo con algoritmos de\n",
    "optimización y registre las pérdidas y tiempos de entrenamiento y test. Debe utilizar al menos 3 diferentes algoritmos.\n",
    "Es decir, procure que su código sea capaz de parametrizar el uso de diferentes algoritmos de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8a79f",
   "metadata": {},
   "source": [
    "#### Task 6 - Experimentación y Análisis\n",
    "\n",
    "Entrene los modelos con diferentes combinaciones de funciones de pérdida, técnicas de regularización y algoritmos\n",
    "de optimización. Para no complicar esta parte, puede dejar fijo dos de estos parámetros (función de pérdida, técnicas de regularización, algoritmo de optimización) y solamente cambiar uno de ellos. Deben verse al menos 9\n",
    "combinaciones en total, donde es válido que en una de ellas no haya ninguna técnica de regularización. Si quiere\n",
    "experimentar con más combinaciones se le dará hasta 10% de puntos extra.\n",
    "Para cada combinación registre métricas como precisión, pérdida y alguna otra métrica que considere pertinente\n",
    "(Recuerde lo visto en inteligencia artificial).\n",
    "Visualice las curvas (tanto en precisión, pérdida y la tercera métrica que decidió) de entrenamiento y validación\n",
    "utilizando bibliotecas como matplotlib y/o seaborn. Además, recuerde llevar tracking de los tiempos de ejecución de\n",
    "cada combinación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd9f19",
   "metadata": {},
   "source": [
    "#### Task 7- Discusión\n",
    "\n",
    "\n",
    "Discuta los resultados obtenidos de diferentes modelos. Compare la velocidad de convergencia y el rendimiento final\n",
    "de modelos utilizando diferentes funciones de pérdida, técnicas de regularización, y algoritmos de optimización.\n",
    "Explore y discuta por qué ciertas técnicas podrían conducir a un mejor rendimiento. tanto técnicas de regularización,\n",
    "funciones de pérdida como algoritmos de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a58169",
   "metadata": {},
   "source": [
    "## Ejercicio 2. Repaso de teoría\n",
    "\n",
    "1. ¿Cuál es la principal innovación de la arquitectura Transformer?\n",
    "\n",
    "    La gran idea del Transformer es que deja de lado las redes recurrentes y las convoluciones y, en su lugar, usa atención para que cada palabra pueda mirar a las demás de una sola vez; así el modelo entiende relaciones lejanas más fácil, entrena en paralelo (más rápido) y solo necesita agregar una pista del orden de las palabras con “positional encodings”.\n",
    "\n",
    "2. ¿Cómo funciona el mecanismo de atención del scaled dot-product?\n",
    "\n",
    "    Cada palabra se compara con todas las otras para ver a cuáles debería “prestarles más atención”; esas comparaciones se convierten en pesos (que suman 1) y se usan para mezclar la información de las palabras importantes. El “escalado” solo evita que los números exploten y el softmax dé resultados raros y en el decodificador se tapa lo que viene después para no hacer trampa.\n",
    "\n",
    "3. ¿Por qué se utiliza la atención de múltiples cabezales en Transformer?\n",
    "\n",
    "    Se usan varias “cabezas” de atención porque cada una puede fijarse en cosas distintas al mismo tiempo: una puede mirar concordancias cercanas, otra dependencias lejanas, otra nombres propios, etc. Al juntar todas, el modelo capta más patrones y entiende mejor el texto que si tuviera una sola mirada.\n",
    "\n",
    "4. ¿Cómo se incorporan los positional encodings en el modelo Transformer?\n",
    "\n",
    "    Como la atención por sí sola no sabe el orden, al embedding de cada palabra se le suma un vector que depende de su posición los positional encodings. Es como ponerle a cada palabra un GPS de “estoy en la posición 1, 2, 3…”, para que el modelo distinga “el perro muerde al gato” de “el gato muerde al perro”.\n",
    "\n",
    "5. ¿Cuáles son algunas aplicaciones de la arquitectura Transformer más allá de la machine translation?\n",
    "\n",
    "    Además de traducir, los Transformers se usan para un montón de cosas: escribir o resumir textos, responder preguntas y chatear, entender y generar código, analizar imágenes, trabajar con audio y voz, combinar texto-imagen, e incluso para datos como secuencias biológicas o series de tiempo. Son versátiles porque la misma idea de atención se adapta a muchos tipos de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepvenv)",
   "language": "python",
   "name": "deepvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
